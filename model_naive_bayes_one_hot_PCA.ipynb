{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier with One-Hot Encoding on a PCA preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (2560, 60000)\n",
      "X_test: (800, 60000)\n",
      "X_val: (640, 60000)\n",
      "X_train: (3200, 60000), y_train: (3200,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "\n",
    "# SADR: path to the dataset.\n",
    "dataset_path = os.path.join(\"preprocessed_datasets\", \"dataset_one_hot.pkl\")\n",
    "\n",
    "# SADR: loading training data.\n",
    "with open(dataset_path, \"rb\") as f:\n",
    "    dataset_one_hot = pickle.load(f)\n",
    "\n",
    "# SADR: getting the training, validation, and testing data.\n",
    "X_train, y_train = dataset_one_hot[\"X_train\"], dataset_one_hot[\"y_train\"]\n",
    "X_val, y_val = dataset_one_hot[\"X_val\"], dataset_one_hot[\"y_val\"]\n",
    "X_test, y_test = dataset_one_hot[\"X_test\"], dataset_one_hot[\"y_test\"]\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "\n",
    "# SADR: rejoining the training and validation data.\n",
    "# Important to do k-fold cross-validation.\n",
    "X_train = np.concat((X_train, X_val))\n",
    "y_train = np.concat((y_train, y_val))\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- GaussianNB -----\n",
      "Mean F1: 0.8615 (±0.0090)\n",
      "Fold scores: ['0.8487', '0.8683', '0.8731', '0.8538', '0.8635']\n",
      "\n",
      "----- MultinomialNB -----\n",
      "Mean F1: 0.9354 (±0.0233)\n",
      "Fold scores: ['0.9446', '0.9416', '0.9521', '0.9492', '0.8894']\n",
      "\n",
      "----- BernoulliNB -----\n",
      "Mean F1: 0.6700 (±0.0141)\n",
      "Fold scores: ['0.6684', '0.6743', '0.6919', '0.6678', '0.6477']\n",
      "\n",
      "\n",
      "Best model: MultinomialNB (Mean F1: 0.9354)\n",
      "\n",
      "=== Test Set Performance ===\n",
      "Test F1 Score: 0.9456\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Combine train and validation sets\n",
    "X_combined = np.concatenate((X_train, X_val), axis=0)\n",
    "y_combined = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "# Convert labels if needed (assuming -1/1 encoding)\n",
    "y_combined = np.where(y_combined == -1, 0, y_combined)\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=120, random_state=42)\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"BernoulliNB\": BernoulliNB()\n",
    "}\n",
    "\n",
    "# Manual 5-fold cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    fold_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_combined):\n",
    "        # Split data\n",
    "        X_train_fold, X_val_fold = X_combined[train_idx], X_combined[val_idx]\n",
    "        y_train_fold, y_val_fold = y_combined[train_idx], y_combined[val_idx]\n",
    "        \n",
    "        # Apply PCA\n",
    "        X_train_pca = pca.fit_transform(X_train_fold)\n",
    "        X_val_pca = pca.transform(X_val_fold)\n",
    "        \n",
    "        # Apply MinMaxScaler for MultinomialNB/BernoulliNB\n",
    "        if name in [\"MultinomialNB\", \"BernoulliNB\"]:\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_pca = scaler.fit_transform(X_train_pca)\n",
    "            X_val_pca = scaler.transform(X_val_pca)\n",
    "        \n",
    "        # Train and predict\n",
    "        model.fit(X_train_pca, y_train_fold)\n",
    "        y_pred = model.predict(X_val_pca)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        fold_f1 = f1_score(y_val_fold, y_pred)\n",
    "        fold_scores.append(fold_f1)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"mean_f1\": np.mean(fold_scores),\n",
    "        \"std_f1\": np.std(fold_scores),\n",
    "        \"all_scores\": fold_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"----- {name} -----\")\n",
    "    print(f\"Mean F1: {results[name]['mean_f1']:.4f} (±{results[name]['std_f1']:.4f})\")\n",
    "    print(f\"Fold scores: {[f'{s:.4f}' for s in fold_scores]}\\n\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(results, key=lambda x: results[x][\"mean_f1\"])\n",
    "print(f\"\\nBest model: {best_model_name} (Mean F1: {results[best_model_name]['mean_f1']:.4f})\")\n",
    "\n",
    "# Train best model on full data with PCA\n",
    "pca_full = PCA(n_components=120, random_state=42)\n",
    "X_combined_pca = pca_full.fit_transform(X_combined)\n",
    "\n",
    "# Apply MinMaxScaler if best model is MultinomialNB/BernoulliNB\n",
    "if best_model_name in [\"MultinomialNB\", \"BernoulliNB\"]:\n",
    "    scaler_full = MinMaxScaler()\n",
    "    X_combined_pca = scaler_full.fit_transform(X_combined_pca)\n",
    "\n",
    "best_model = models[best_model_name].fit(X_combined_pca, y_combined)\n",
    "\n",
    "# Evaluate on test set\n",
    "X_test_pca = pca_full.transform(X_test)\n",
    "if best_model_name in [\"MultinomialNB\", \"BernoulliNB\"]:\n",
    "    X_test_pca = scaler_full.transform(X_test_pca)\n",
    "y_test = np.where(y_test == -1, 0, y_test)  # Convert labels if needed\n",
    "y_pred = best_model.predict(X_test_pca)\n",
    "\n",
    "print(\"\\n=== Test Set Performance ===\")\n",
    "print(f\"Test F1 Score: {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Set Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.94      0.94       386\n",
      "     Class 1       0.95      0.94      0.95       414\n",
      "\n",
      "    accuracy                           0.94       800\n",
      "   macro avg       0.94      0.94      0.94       800\n",
      "weighted avg       0.94      0.94      0.94       800\n",
      "\n",
      "Accuracy: 0.9437\n",
      "Precision: 0.9467\n",
      "Recall: 0.9444\n",
      "F1-Score: 0.9456\n",
      "Support: [386 414]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Ensure test labels are in {0, 1} (if encoded as -1/1)\n",
    "y_test = np.where(y_test == -1, 0, y_test)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_model.predict(X_test_pca)\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "support = np.bincount(y_test)  # Counts of each class in test set\n",
    "\n",
    "# Print detailed report\n",
    "print(\"\\n=== Test Set Evaluation ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Class 0\", \"Class 1\"]))\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Support: {support}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 10 Most Important PCA Components ===\n",
      "   PC_Component  Importance_Score  Class_0_LogProb  Class_1_LogProb\n",
      "0             9          0.690239        -5.239005        -5.929245\n",
      "1             1          0.612279        -4.747316        -4.135037\n",
      "2            10          0.290542        -5.227305        -5.517847\n",
      "3             0          0.283684        -4.791873        -4.508189\n",
      "4             2          0.170034        -4.500307        -4.330273\n",
      "5             8          0.060897        -4.978840        -5.039737\n",
      "6             6          0.045046        -4.640229        -4.595183\n",
      "7            20          0.044210        -4.740774        -4.784983\n",
      "8            15          0.042181        -4.792479        -4.750298\n",
      "9            26          0.040056        -5.048699        -5.088755\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Assuming you've already trained your best_model (MultinomialNB) and have X_test_pca\n",
    "\n",
    "# Get feature log probabilities from the trained model\n",
    "# These are log(P(feature|class))\n",
    "log_prob = best_model.feature_log_prob_  # Shape: (n_classes, n_features)\n",
    "\n",
    "# Calculate the absolute difference between class log probabilities\n",
    "# This shows which features have the largest difference between classes\n",
    "feature_importance = np.abs(log_prob[1] - log_prob[0])\n",
    "\n",
    "# Get indices of top N most important features\n",
    "top_n = 10  # Number of top features you want to see\n",
    "top_indices = np.argsort(feature_importance)[-top_n:][::-1]\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'PC_Component': top_indices,\n",
    "    'Importance_Score': feature_importance[top_indices],\n",
    "    'Class_0_LogProb': log_prob[0][top_indices],\n",
    "    'Class_1_LogProb': log_prob[1][top_indices]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Top {} Most Important PCA Components ===\".format(top_n))\n",
    "print(importance_df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
